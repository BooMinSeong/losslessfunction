---
title: "Sample Paper Review"
date: 2025-12-16T13:10:03Z
draft: false
categories: ["papers"]
tags: ["deep learning", "computer vision"]

# ë…¼ë¬¸ ì •ë³´
paper:
  title: "Attention is All You Need"
  authors: ["Vaswani et al."]
  venue: "NeurIPS"
  year: 2017
  arxiv_id: "1706.03762"
  url: "https://arxiv.org/abs/1706.03762"

# AI ì¹œí™”ì  ë©”íƒ€ë°ì´í„°
difficulty: "intermediate"
summary: "Transformer ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆí•œ íšê¸°ì ì¸ ë…¼ë¬¸"
keywords: ["transformer", "attention", "sequence-to-sequence"]
---

## ğŸ“„ ë…¼ë¬¸ ì •ë³´

- **Title**: Attention is All You Need
- **Authors**: Vaswani et al.
- **Venue**: NeurIPS 2017
- **Year**: 2017
- **Link**: https://arxiv.org/abs/1706.03762

## ğŸ¯ í•µì‹¬ ìš”ì•½

ì´ ë…¼ë¬¸ì€ RNNì´ë‚˜ CNN ì—†ì´ ì˜¤ì§ attention ë©”ì»¤ë‹ˆì¦˜ë§Œì„ ì‚¬ìš©í•˜ëŠ” Transformer ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. Self-attentionì„ í†µí•´ ì‹œí€€ìŠ¤ì˜ ëª¨ë“  ìœ„ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆì–´ í•™ìŠµ ì†ë„ê°€ ë¹ ë¥´ê³ , ê¸´ ì˜ì¡´ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ” ì£¼ìš” ë‚´ìš©

### ë¬¸ì œ ì •ì˜

ê¸°ì¡´ RNN ê¸°ë°˜ ëª¨ë¸ì€ ìˆœì°¨ì  ì²˜ë¦¬ë¡œ ì¸í•´ ë³‘ë ¬í™”ê°€ ì–´ë µê³ , ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ì˜ì¡´ì„± í•™ìŠµì´ ì–´ë ¤ì› ìŠµë‹ˆë‹¤.

### ì œì•ˆ ë°©ë²•

Multi-head self-attentionê³¼ position-wise feed-forward ë„¤íŠ¸ì›Œí¬ë¡œ êµ¬ì„±ëœ encoder-decoder êµ¬ì¡°ë¥¼ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

### ì‹¤í—˜ ê²°ê³¼

ê¸°ê³„ ë²ˆì—­ íƒœìŠ¤í¬ì—ì„œ SOTAë¥¼ ë‹¬ì„±í•˜ë©´ì„œë„ í•™ìŠµ ì‹œê°„ì„ í¬ê²Œ ë‹¨ì¶•í–ˆìŠµë‹ˆë‹¤.

## ğŸ’­ ê°œì¸ í‰ê°€

### ì¥ì 

ë³‘ë ¬ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•˜ì—¬ í•™ìŠµ íš¨ìœ¨ì´ ë†’ê³ , í˜„ëŒ€ NLPì˜ ê¸°ë°˜ì´ ë˜ì—ˆìŠµë‹ˆë‹¤.

### í•œê³„

ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§ì•„ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ì ìš© ê°€ëŠ¥ì„±

NLPë¿ë§Œ ì•„ë‹ˆë¼ ë¹„ì „, ìŒì„± ë“± ë‹¤ì–‘í•œ ë„ë©”ì¸ì— ì ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

## ğŸ”— ê´€ë ¨ ìë£Œ

- [Original Paper](https://arxiv.org/abs/1706.03762)
- [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
